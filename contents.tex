% http://wiki.eclipse.org/Development_Resources/HOWTO/Bugzilla_Use
%   The severity tags aren't used much, except to distinguish enhancements from bugs. Typically the users specify severities as they see fit.

% http://wiki.eclipse.org/Bug_Reporting_FAQ
%   when a test pass comes up a component owner can build a query that searched for resolved bugs
%   After testing that the fix worked a resolved bug can be transitioned to verified, directly to closed, or in fact, reopened. 
%   By searching for bugs with a Status of verified and a resolution of fixed developers can come up with release notes.
%   Severity is assigned by a user and describes the level of impact the bug is having on them. Priority is assigned by the developer and describes the importance a developer places on fixing the bug.

\newcommand{\UNCONFIRMED}{\texttt{UNCONFIRMED}}
\newcommand{\NEW}{\texttt{NEW}}
\newcommand{\ASSIGNED}{\texttt{ASSIGNED}}
\newcommand{\VERIFIED}{\texttt{VERIFIED}}
\newcommand{\RESOLVED}{\texttt{RESOLVED}}
\newcommand{\CLOSED}{\texttt{CLOSED}}
\newcommand{\REOPENED}{\texttt{REOPENED}}
\newcommand{\FIXED}{\texttt{FIXED}}

\begin{section}{Introduction}

Bug repositories have for a long time been used in software projects to support coordination among stakeholders. They record discussion and progress of software evolution activities, such as bug fixing and software verification. Hence, bug repositories are an opportunity for researchers who intend to investigate issues related to the quality of both the product and the process of a software development team.
%, or even issues regarding the nature of software development itself.

% The information contained in bug repositories has been used in order to predict fault-proneness, to unveil developers' roles from data, to characterize transfer of work in software projects, and so on. Also, it has been used to investigate beliefs in software engineering related to software quality, such as the impact of developer turnover and of collective ownership on the occurrence of bugs.

However, mining bug repositories has its own risks. Previous research has identified problems of missing data (e.g., rationale, traceability links between reported bug fixes and source code changes), inaccurate data (e.g., misclassification of bugs) \cite{Aranda2009}, and biased data \cite{Bird2009}. %,Bird2009,Nguyen2010,Ayari2007}.

  In previous research \cite{Souza2011}, we tried to assess the impact of independent verification of bug fixes on software quality, by mining data from bug repositories. 
  We relied on reported verifications tasks, as recorded in bug reports, and interpreted the recorded data according to the documentation for the specific bug tracking system used.
  As the partial results suggested that verification has no impact on software quality, we questioned the accuracy of the data about verification of bug fixes, and thus decided to investigate how verification is actually performed and reported on the projects that were analyzed, Eclipse and NetBeans.
	
	Hence, in this paper, we investigate the following exploratory research questions regarding the software verification process in Eclipse and NetBeans:

	\begin{itemize}
		\item \textbf{When} is the verification performed: is it performed just after the fix, or is there a verification phase?
		\item \textbf{Who} performs the verification: is there a QA (quality assurance) team? 		
		\item \textbf{How} is the verification performed: are there performed ad hoc tests, automated tests, code inspection?
		% \item How is the distribution of verification across developers?
		% \item Is there a dedicated team for QA?
	\end{itemize}

	The next section contains some background information about bug tracking systems and software verification. In Section \ref{sec:methods}, the data and methods used to investigate the research questions are presented. Then, in Section \ref{sec:results}, the results are exposed and discussed. Finally, Section \ref{sec:conclusion} presents some concluding remarks.

% In summary, for researchers who intend to use data from bug tracking systems, it is not sufficient to understand the data fields in a bug report the way they are documented in the system's documentation. The research should also investigate whether the data fields are used often, whether they are used accordingly to the documentation, and even if the data fields are used consistently across the developers.

% In this paper, we investigate the status of a bug, a data field present in most bug tracking systems that reports the progress of the bug solving activity. In particular, we focus on the activity of verification of a fix. We are interested in answering the following exploratory research questions:

% For research question xx, qualitative of a small sample. 
% For research question xx, all bugs were considered.

\end{section}

\begin{section}{Background} \label{sec:background}
% Bug workflow. Bugzilla. \cite{Bird2009}

% Reposit ́orios de bugs. Arquivam relat ́orios de defeitos e solicitac ̧ ̃oes de mudan ̧ca. Tais reposito ́rios ajudam os desenvolvedores a registrar, discutir e acompanhar a situa ̧ca ̃o de cada solicita ̧c ̃ao, desde a atribui ̧ca ̃o de uma solicita ̧ca ̃o a um desenvolvedor at ́e a verifica ̧c ̃ao da corre ̧c ̃ao ou melhoria. Exemplos incluem o Bugzilla, o Trac e o Jira.

Bug tracking systems allow users and developers of a software project to manage a list of bugs for the project, along with information such as steps to reproduce the bug, its severity and the operating system used. Developers choose bugs to fix and can report on the progress of the bug fixing activities, ask for clarification, discuss causes for the bug etc.

In this research, we focus on Bugzilla, an open source bug tracking system used by notable software projects such as Eclipse, Mozilla, Linux Kernel, NetBeans, Apache, and companies such as NASA and Facebook\footnote{Complete list available at http://www.bugzilla.org/installation-list/.}. The general concepts from Bugzilla should apply to most other bug tracking systems.

One important feature of a bug that is recorded on bug tracking systems is its status. The status records the progress of the bug fixing activity.
%, and, as such, provides data to software engineering studies
Figure \ref{fig:bugzilla} shows each status that can be recorded in Bugzilla, along with typical transitions between status values, i.e., the workflow.

\begin{figure}[ht]
	\centering
		\includegraphics[scale=0.4]{bugzilla.png}
	\caption{Workflow for Bugzilla. Source: \url{http://www.bugzilla.org/docs/2.18/html/lifecycle.html}.}
	\label{fig:bugzilla}
\end{figure}

In the simplest cases, a bug is created and receive the status \UNCONFIRMED\ (if it was created by a regular user) or \NEW\ (if it was created by a developer). Next, it is \ASSIGNED\ to a developer, and then it is \RESOLVED, possibly by fixing it with a patch on the source code. The solution is then \VERIFIED\ by someone in the quality assurance team, if it is adequate, or otherwise it is \REOPENED. When a version of the software is released, all \VERIFIED\ bugs are \CLOSED.

% If it passes the quality requirements, it is \CLOSED\ when the next release of the software is released. If it does not pass the quality requirements or if the solution only partially fixes the problem, the bug is \REOPENED.

% Roles: reporter, fixer, verifier, reopener, closer.

% bugs with status \VERIFIED\ are used to write the release notes / change log.

% In order to understand the bug fixing process of a software team, however, it should be possible to know what activities are performed upon change of status. 
%For example, what steps are made before changing the status of a bug to RESOLVED (FIXED)? Is the patch applied to the version control system? Is it presented as diff file attached to the bug report? Is there a build available to end users containing the patch (e.g., a nightly build)? The documentation for Bugzilla does not prescribe specific activities for marking a bug as RESOLVED, relegating instead this decision to each software project.
%
%Similarly, w
%What do developers do before marking a bug as \VERIFIED? 
Bugzilla documentation states that, when a bug is \VERIFIED, it means that ``QA [quality assurance team] has looked at the bug and the resolution and agrees that the appropriate resolution has been taken''\footnote{\url{https://landfill.bugzilla.org/bugzilla-3.6-branch/page.cgi?id=fields.html}}. It does not specify how developers should look at the resolution (e.g., by looking at the source code, or by running a patched version of the software). 

% Again, the definition is broad, although it assumes the existence of a QA team.

% The developer documentation for both Eclipse and NetBeans are unclear about how the verification of a bug fix is performed. So, what are the forms of software verification used in the verification process of these projects?

% In some parts they mention running unit tests, and running a patched version of the software and try to reproduce the bug.

Software verification techniques are classified in static and dynamic \cite{Sommerville1995}. Static techniques include \emph{source code inspection}, automated \emph{static analysis}, and \emph{formal verification}. Dynamic techniques, or testing, involve executing the software system under certain conditions and comparing its actual behavior with the intended behavior. Testing can be done in an improvised way (\emph{ad hoc testing}), or it can be structured as a list of test cases, leading to \emph{automated testing}.

% While mining bug repositories, one cannot assume that the \VERIFIED\ status comprises all forms of software verification. Also, one cannot rely on the information provided by the bug tracking system or the developer documentation for a project, since it can be too generic to be useful. Such information can be assessed by taking a closer look on actual bug reports.

% O bug não se manifesta após a aplicação da solução.
% A solução não gera novos bugs.
% O código da solução segue as convenções de codificação e políticas do projeto.
% A solução possui uma boa relação custo-benefício — isto é, ou não existem soluções melhores ou a solução é um paliativo por conta da alta complexidade de uma solução melhor.
% 
% Minha expectativa, portanto, é que, antes de o bug ser marcado como VERIFICADO, ele passe pelas seguintes atividades:
% 
% Aplicação do patch com a solução do bug na versão do software onde o bug foi encontrado.
% Execução do software com o bug, seguindo os passos para reprodução do bug, e verificando que o bug não se manifesta
% Inspeção do código-fonte. Deve ser verificada a aderência às convenções de codificação do projeto e a outras políticas (por exemplo, cada patch deve vir com testes de unidade).
% Execução de testes de unidade (se houver). Deve se verificar que todos os testes passam.

\end{section}

\begin{section}{Method} \label{sec:methods}

	% \item How is the verification actually performed?
	% \item When is the verification performed (i.e., just after the fix, or is there a specific period for the verification of fixed bugs?)
	% \item Who performs the verification?

	In order to answer the research questions---when and how bug fixes are verified, and who verifies them---, a three-part method was used:
	
	\begin{enumerate}
		\item \textbf{Data extraction}: we have obtained publicly available raw data from the Bugzilla repositories of two popular integrated development environments, Eclipse and NetBeans.

		\item \textbf{Data sampling}: for each project, two representative subprojects were chosen for analysis.
		
		% we have chosen two subprojects. After that, we sampled bugs with interesting characteristics.
		
		% \item \textbf{Informal analysis}: some bug reports were fully read, so insights could be gained from them.
		
		\item \textbf{Data analysis}: for each research question, a distinct analysis was required. The analyses will be further described.
	\end{enumerate}	
	
	The experimental package is available at \\ \url{https://sites.google.com/site/rodrigorgs2/msr2012}

\begin{subsection}{Data Extraction}
	
	In order to perform the desired analyses, we needed access to the data recorded by Bugzilla for a specific project, including status changes and comments. We have found such data for two projects---Eclipse and NetBeans---from the domain of integrated development environments. The data was made available as part of the Mining Software Repositories 2011 Challenge\footnote{\url{http://2011.msrconf.org/msr-challenge.html}} in the form of MySQL database dumps. The files contain all data from the respective databases, except for developer profiles, omitted for privacy reasons.
	
	% We have had access to Eclipse Bugzilla database from October, 2001 to June, 2010, and to NetBeans Bugzilla database from ... to .... The data was made available as part of the Mining Software Repositories 2011 Challenge\footnote{\url{http://www.msrconf.org/msr-challenge.html}} as a SQL dump. The SQL dump contains all information recorded by Bugzilla for the projects (except for information about developers, which was removed for privacy reasons).

	Eclipse development began in late 1998 with IBM\footnote{\url{http://www.ibm.com/developerworks/rational/library/nov05/cernosek/}}. It was licensed as open source in November, 2001. The available data set contains 316,911 bug reports for its 155 subprojects, from October, 2001 to June, 2010.
		
	NetBeans\footnote{\url{http://netbeans.org/about/history.html}} started  as a student project in 1996. It was then bought by Sun Microsystems in October, 1999, and open sourced in March, 2000. The data set contains 185,578 bug reports for its 39 subprojects, from June, 1998 to June, 2010.
	
\end{subsection}

\begin{subsection}{Data Sampling}

	Four subprojects were chosen for further analysis: Eclipse/Platform, Eclipse/EMF, NetBeans/Platform, and NetBeans/VersionControl. The Platform subprojects are the main subprojects for the respective IDEs, so they are both important and representative of each projects' philosophy. 
	
	The other two subprojects were chosen at random, restricted to subprojects in which the proportion of verified bugs was greater than the proportion observed in the respective Platform subprojects. The reason is to avoid selecting projects in which bugs are seldom marked as \VERIFIED. The following proportions of \VERIFIED\ bugs per project were observed: Eclipse/Platform: 16.0\%; Eclipse/EMF: 48.4\%; NetBeans/Platform: 21.4\%; NetBeans/VersionControl: 29.7\%.
	
	% NetBeans/VersionControl: 0.29694479
	% NetBeans/Platform: 0.21423101
	% Eclipse/EMF: 0.4835361489
	% Eclipse/Platform: 0.1595397738
	
	% In order to obtain insight about the research questions, we read a sample of bug reports from both projects. For each project, two subprojects were chosen randomly among the projects with most verified bugs, resulting in the following subprojects: Eclipse/Platform, Eclipse/EMF, NetBeans/platform, NetBeans/versioncontrol. We have sampled 20 bug reports for each subproject, totalizing 80 bug reports.
	% 
	
\end{subsection}

\begin{subsection}{Analysis: When Are Bugs Verified?}
	
	In order to determine if there is a well-defined verification phase for the subprojects, we have selected all reported verifications (i.e., status changes to \VERIFIED) over the lifetime of each subproject. Then, we have plotted, for each day in the interval, the accumulated number of verifications reported since the first day available in the data. The curve is monotonically increasing, with steeper ascents representing periods of intense verification activity.
	
	Also, we have obtained the release dates for multiple versions of Eclipse and NetBeans. The information was obtained from the respective websites. In cases in which older information was not available, archived versions of the web pages were accessed via the website \url{www.archive.org}.
	
	If a subproject presents a well-defined verification phase, it is expected that the verification activity is more intense a few days before a release. Such pattern can be identified by visual inspection of the graph, by looking for steeper ascents in the verification curve preceding the release dates.
	
\end{subsection}

\begin{subsection}{Analysis: Who Verifies Bugs?}
	
	In order to determine whether there is a team dedicated to quality assurance (QA), we have counted how many times each developer has marked a bug as \FIXED\ or \VERIFIED. We considered that a developer is part of a QA team if s/he verified at least 10 times (i.e., one order of magnitude) more than s/he fixed bugs. Also, we have computed the proportion of verifications that was performed by the discovered QA team. It is expected that, if the discovered set of developers is actually a QA team, they should be responsible for the majority of the verifications.
	
\end{subsection}

\begin{subsection}{Analysis: How Are Bugs Verified?}

	In order to discover the verification techniques used by the subprojects, we have looked at the comments written by developers when they mark a bug as \VERIFIED\ (meaning that the fix was accepted) or \REOPENED\ (meaning that the fix was rejected). First, an informal analysis was performed, by reading a sample of such comments, looking for textual patterns that indicate that a particular technique was used. Then, based on the informal analysis and on previous knowledge, a regular expression was built for each technique, to match comments in which there is evidence that a particular technique was used. The regular expressions were refined by looking at the results matched by them and modifying them to avoid false positives. 
	
	The complete regular expressions are available in the experimental package. For the sake of simplicity, the following list describes only some substrings matched by the regular expressions for each technique:
		
	\begin{itemize}
		\item automated testing: testcase, test case, unit test, automated test, test result, run the test, test run;
		\item source code inspection: inspecting, inspection, look at the code, looking at the source;
		\item ad hoc testing: verified by running, verified by [word ending in ``ing''];
		\item automated static analysis: findbugs, checkstyle, static code, static analys;
		\item formal testing: formal test, formal verif.
	\end{itemize}
	
	% The classification script is smart enough to ignore negative assertions, such as ``did not run the test''.	
	
	During the informal analysis, it was observed that ad hoc testing is reported in many distinct ways that are difficult to capture by means of regular expressions. Hence, its regular expression is expected to match only a few comments.
	
	% The regular expression for ad hoc testing is the most restrictive of them, because ad hoc testing was found to be reported in many distinct ways (when it is reported). %We expect to find fewer examples of ad hoc testing than it should be found.
	
\end{subsection}

\end{section}

\begin{section}{Results and Discussion} \label{sec:results}
	
	Each of the three analysis was performed on Eclipse/Platform, Eclipse/EMF, NetBeans/Platform, and NetBeans/VersionControl. The results are presented next.
	
% \begin{subsection}{Preliminary Analysis}
% 	Some insights were gained in this step, partially answering the who, when, and how questions:
% 	
% 	\begin{itemize}
% 		\item [how] (OK) In some projects, the status VERIFIED does not correspond to software verification. In Eclipse/EMF, for example, it means that the fix is available in a build that is promoted on the project website\footnote{\url{http://wiki.eclipse.org/Modeling_PMC_Meeting,_2007-10-16} (between releases 3.3.1 and 3.3.1.1)}. Further evidence: bug #254489 ``Please do not change the state of the bugs since the status verified fixed should be only there when we publish a new build. :-)''
% 		\item [(who)] (NOT RELEVANT) In the NetBeans subprojects that were analyzed, it was common that the reporter also verified the fix. In some bugs, this was necessary since the developer was not able to reproduce the bug.
% 		\item [(who, when)] Often, the fixer verifies the bug just after marking it as fixed.
% 		\item [(when)] (OK) Sometimes, several old bugs are marked as VERIFIED at the same time.
% 		\item [(how?)] (OK) In both Eclipse and NetBeans, it is common that the fixer requests someone else to verify the fix (``Please verify, Tom'').
% 		\item [(how)] (OK) In Eclipse/EMF, bug #249436, one developer rewrites the patch submitted by other developer.
% 		\item [(how)] (OK) In Eclipse/EMF, bug #269789, a test case is used to assess the proposed fix.
% 		\item [(how)] (OK) Eclipse/Platform, bug #210533: ``Verified by code inspection''.
% 	\end{itemize}
% 	
% \end{subsection}	
	
\begin{subsection}{When Are Bugs Verified?}
	
	% In order to answer whether there is a verification phase for the projects analyzed, we plotted the cumulative number of verifications along the lifetime of each project, as shown in Figure XXX, in red. The vertical lines represent release dates for each version. As a basis for comparison, the number of bug fixes is also plotted, in black. 

	Figure \ref{fig:when} shows plots of total accumulated number of verifications over time for Eclipse/Platform (left) and NetBeans/Platform (right). Releases are plotted as dashed vertical lines.	

\begin{figure}
\centering
\mbox{
\subfigure{\includegraphics[width=0.25\textwidth]{milestones}}%\quad
\subfigure{\includegraphics[width=0.25\textwidth]{cliffs} }}
\caption{Number of verifications over time.} \label{fig:when}
\end{figure}

% \begin{figure}[htbp]
% 	\centering
% 		\includegraphics[scale=1]{file}
% 	\caption{caption}
% 	\label{fig:label}
% \end{figure}

	For Eclipse/Platform, the graph shows the period between releases 3.5 and 3.6, including milestone releases every 6 or 7 weeks. It is clear from the graph that the verification activity is intensified in a few days preceding a milestone, represented in the graph by steeper ascents before the vertical lines. This pattern is an indicator of a verification phase. No such pattern was found in the other subprojects by analyzing their graphs (not shown here for brevity).

	The graph for NetBeans/Platform (Figure \ref{fig:when}, right side) shows the entire project history. Although there are steeper ascents, they are different because they do not precede release dates. Also, at a closer look, they represent thousands of verifications performed in a few minutes by the same developer, with the same comment. The same pattern was found in Eclipse/EMF (not shown).
	
	Of course, no developer can verify so many fixes in so little time. The explanation, supported by the comments, is that such mass verifications represent some cleanup of the bug repository, by marking old bugs as \VERIFIED---with no verification being actually performed. Researchers should take extra care with mass verifications, as they contain a large amount of bugs and, thus, are likely to bias the results of analyses.
	
	% 
	% We found evidence of a verification phase for Eclipse/Platform. In this project, verifications appear to be performed in bursts, with periods of intense activity separated by about 6 to 7 weeks of low or no activity. By taking a closer look in the period between the releases 3.5 and 3.6 (Figure XXX), it is clear that the periods of intense verification activity occur just before the milestones leading to 3.6.
	% 
	% No such pattern was found for NetBeans' subprojects.
	
	% We did not find evidence of a verification phase preceding each release. Unlike the bug fixing activity, the pace of verification is not steady, either. Instead, verification appears to be performed in bursts, which a couple of weeks with low or no activity, followed by a couple of days of intense verification activity.
	
	% In Eclipse, milestones are roughly 6 weeks apart from each other. Milestones are considered good enough for end users\footenote{\url{http://eclipsecon.org/2005/presentations/econ2005-eclipse-way.pdf}} The milestones are, in general, marked by a steep accent in verification just before the milestone date.
	% Weekly integration builds. Nightly builds. Build quality verified by automatic unit tests. 20k unit tests.
	
	%%%%%%%%%%%%%%%%%%%%%%
	
	% The first thing that comes to attention are ``cliff walls'', points in time when a large amount of bugs are verified, represented by steep, almost vertical portions of the plotted line. They appear in Eclipse/EMF and NetBeans/Platform. In NetBeans/Platform, all verifications in the cliff wall preceding version 5.5 share the same message: ``Verification of old issues''. For Eclipse/EMF, the message is ``Move to verified as per bug 206558'', which refers to the task of changing bugs marked as \FIXED\ to \VERIFIED.
	% 
	% Cliff walls represent a change in the development process, and the mass verification is needed in order to leverage useful features from Bugzilla (e.g., looking for verified bugs in order to aid the writing of release notes).
	
	% Because of cliff walls, extra care must be taken before doing any analysis that relies on the \VERIFIED\ status. In such situations, the \VERIFIED\ status is applied blindly, with no software verification being actually performed. Because cliff walls contain a large amount of bugs, they are likely to bias the results of analyses.
	
	%%%%%%%%%%%%%%%%%%%%%%
	
	% Some projects have ``cliff walls'' [reference graphic]. Taking a closer look, the VERIFIED status was used sparingly in Eclipse/EMF, but, suddenly, a huge number of bugs is verified and, after that, bugs tend to be verified frequently. It represents a change in the development process, and the mass verification is needed in order to leverage useful features from Bugzilla (e.g., looking for verified bugs in order to aid the writing of release notes).
	
	% If one is to build a predictive model based on verifications, the cliff walls should be ignored, as they represent unusual activity that has no connection with software verification whatsoever.
	
	% By visualizing release dates, we tried to assess if there is such a thing as a ``verification phase'', i.e., if verifications are frequent only on a period before the release. There was no evidence of such phase on Eclipse subprojects (TODO: look for patterns in NetBeans).
	
	% However, we found also that, different to what occur with fixes, verifications are not steady, constant. In Eclipse/Platform, for instance, there are periods of 3 to 5 weeks of few verifications, followed by a couple of days with intense verification activity (``mini cliff walls'', if you wish...)
	% 
	% The verifications of NetBeans/versioncontrol are more steady, continuous. Also, they seem to reflect well the FIXED curve.
	
	% TODO (future): are verifications more common on mondays on netbeans-versioncontrol?
	
	% TODO (future): how much time does it take between RESOLVED (FIXED) and VERIFIED?
	
\end{subsection}

\begin{subsection}{Who Verifies Bugs?}

	Table \ref{tab:qa} presents, for each subproject, the number of developers attributed to a QA team (i.e., developers who perform verifications at least 10 times more often than contribute bug fixes), and the proportion of bug verifications they account for. Mass verifications and developers who have not contributed with fixes and verifications are discarded from the analysis.
	
	\newcommand\MyHead[2]{%
	  \multicolumn{1}{l}{\parbox{#1}{\centering #2}}
	}
	
	\begin{table}
		\begin{center}
		
		\caption{Discovered QA team for all four subprojects.} \label{tab:qa}
		\begin{tabular}{l|r|r}
			% \toprule
			\textbf{\centering Project} & 
			\textbf{\centering QA team size} & 
			\MyHead{1.5cm}{\textbf{\% of verifications by QA team}} \\
			\hline
			Eclipse & & \\
			\quad Platform        & 4 (2.4\%)  &  1.1\% \\
			\quad EMF             & 0 (0.0\%)  &  0.0\% \\
			NetBeans & & \\
			\quad Platform       & 25 (18.8\%) & 80.1\% \\
			\quad VersionControl & 5 (20.8\%) & 93.2\% \\
			% \bottomrule
		\end{tabular}
		\end{center}

	\end{table}
		
	In both NetBeans subprojects it is possible to infer the existence of a QA team, composed by approximately 20\% of the developers, performing at least 80\% of all verifications. In the Eclipse subprojects, there is no evidence of a dedicated QA team. In Eclipse/EMF no developer focus specifically on verification. In Eclipse/Platform, 2.4\% of the developers focus on verification tasks, contributing only to 1.1\% of the verifications.
	
	Although the reported results are based on an arbitrary threshold (10) for the ratio between verifications and fixes, other values (2 and 5) were also used, leading to similar results.
	
% (it should be remembered that verification in EMF has a peculiar meaning)	
	
\end{subsection}

\begin{subsection}{How Are Bugs Verified?}

  % To identify the verification techniques used by the projects, we looked at messages written by developers when they mark a bug as VERIFIED (meaning that the bug fix is appropriate) or REOPENED (meaning that some problem was found in the proposed fix). First, we have read some messages, non-systematically, to get some idea of what is said in theses messages and how it is said.

	Table \ref{tab:how} shows, for each subproject, the number of comments associated with a bug being marked as \VERIFIED\ or \REOPENED\ that refers to a particular verification technique. No references to formal verification or static analysis were found; therefore, these techniques are not shown on the table.

	\begin{table}
		\begin{center}
		\caption{Verification techniques for all four subprojects.} \label{tab:how}
		\begin{tabular}{l|r|r|r}
			% \toprule
			\textbf{\centering Project} & 
			\textbf{\centering Testing} & 
			\textbf{\centering Inspection} & 
			\textbf{\centering Ad Hoc} \\
			\hline
			Eclipse/Platform        & 260 (1.1\%) & 524 (2.2\%) & 74 (0.3\%) \\
			Eclipse/EMF             & 27 (0.7\%) & 2 (0.1\%) & 0 (0.0\%) \\
			NetBeans/Platform       & 94 (0.6\%) & 5 (0.0\%) & 0 (0.0\%) \\
			NetBeans/VersionControl & 4 (0.1\%) & 1 (0.0\%) & 0 (0.0\%) 
			% \bottomrule
		\end{tabular}
		\end{center}
	\end{table}

	In Eclipse/Platform, less than 4\% of the comments referenced the verification technique that was used; for the other projects, less than 1\% of the comments contained such references. Such low proportion reveals how difficult it is to infer the verification technique that was applied using only data from bug repositories. This is probably tacit knowledge that can only be extracted by asking the developers themselves.
	
	In all subprojects, comments suggest the use of automated testing and code inspection in the verification process, the former technique being more frequently referenced (except in Eclipse/Platform, in which code inspection is cited more often). Evidences of ad hoc testing were found only in Eclipse/Platform, because verification comments tend to be more structured in this project, but we believe that ad hoc testing is much more common, given its unstructured nature.
	
	While sorting through comments, we have found interesting information regarding the software verification process. Most verification comments do not provide any clue about the verification technique used. Most often, developers just state that the bug fix was verified, sometimes informing the build used to verify the fix. 
  
	In Eclipse/Platform, the developer who fixes a bug often asks someone else to verify the fix, by asking ``please verify, [developer name]''. If the bug is reopened, then the fixer and the verifier exchange roles. This behavior illustrates a structured bug fixing/verification process.
	
	In Eclipse/EMF, we found that marking a bug as \VERIFIED\ does not mean that the bug fix was verified. Instead, it means that the fix was made available in a build of the software that is published in the subproject's website. This reinterpretation of the Bugzilla workflow was decided in a meeting\footnote{\url{http://wiki.eclipse.org/Modeling_PMC_Meeting,_2007-10-16}}.

  % In Eclipse/Platform, though, a significant number of verification messages include the technique used, e.g., ``verified by code inspection'', or ``verified with test case''. Based on such observations, we have built a search term for inspections and another one for unit tests. Then, we have classified the technique described in each verification message using the search term.

	%   In Eclipse/Platform, there are 13,483 verifications, each one with a message. By the search criteria, 494 were identified as code inspections, 118 were identified as tests and 74 were identified as ad hoc.
	% 
	% Evidence of both code inspection and automated testing was found in all projects, although the evidence was weaker.
	
	% \textbf{Observations}.
	%   We also noted that, in Eclipse/Platform, often the fixer requests someone else to verify the fix, by asking ``please verify, [developer name]''. If the bug is reopened, then the fixer and the verifier exchange roles. That is considered good practice.
	% 
	%   It is difficult to identify ad hoc cases, because they are so pervasive that they are not reported, and also they are reported in many different ways.

	% Eclipse/EMF: VERIFIED = in a build available in the website.

	%We analyzed cases in which the bug is fixed, the fixer asks someone else to verify the fix, and then the bug is reopened.

% \begin{itemize}
% 	\item By creating or running unit tests.
% 	\item By creating throw-away or configuration code that forces the occurrence of the error.
% 	\item By running the software in a determined way (ad hoc). Often indicated by verbs.
% 	\item By inspecting code.
% \end{itemize}

	% Conclusion: it's hard to determine the technique used by just looking at the data. Eclipse/Platform documents it better, sometimes.
	
\end{subsection}

\end{section}

\begin{section}{Conclusion} \label{sec:conclusion}
	By analyzing four subprojects (two from Eclipse, two from NetBeans), we have found, using only data from bug repositories, subprojects with and without QA teams, with and without a well-defined verification phase. We also have found weaker evidence of the application of automated testing and source code inspection. Also, there were cases in which marking a bug as \VERIFIED\ did not imply that any kind of software verification was actually performed.
			
	With the knowledge obtained from this exploratory research, we aim to improve and extend our previous work on the impact of independent verification on software quality. We can investigate, for example, whether verification performed by QA team is more effective than verification performed by other developers.
	
	Researchers should be aware that information about verification techniques may not be common in bug repositories, and that reported verification does not always correspond to actual verification. Some exploration of the data is important to avoid such pitfalls.
	
	% \begin{itemize}
	% 	\item Does independent verification improve software quality when it is performed by developers from the QA team?
	% 	\item Does independent verification improve software quality when it is performed as an separate phase of the software release cycle?
	% 	\item Does independent verification improve software quality when a particular verification technique (e.g., inspections, unit testing) is used?
	% \end{itemize}
	
\end{section}

\section*{Acknowledgment}

\small{This work is supported by FAPESB (grant BOL0119/2010) and CNPq/INES (grant 573964/2008-4).}
